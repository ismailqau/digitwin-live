syntax = "proto3";

package llm;

// LLM Service - Large Language Model
service LLMService {
  // Generate a response with streaming
  rpc GenerateResponse(GenerateRequest) returns (stream GenerateResponse);
  
  // Health check
  rpc HealthCheck(HealthCheckRequest) returns (HealthCheckResponse);
}

message GenerateRequest {
  string session_id = 1;
  string user_id = 2;
  string query = 3;
  string context = 4;
  string personality = 5;
  LLMConfig config = 6;
}

message LLMConfig {
  string provider = 1;
  string model = 2;
  float temperature = 3;
  int32 max_tokens = 4;
  repeated string stop_sequences = 5;
}

message GenerateResponse {
  string session_id = 1;
  string token = 2;
  bool is_final = 3;
  string full_response = 4;
  TokenMetrics metrics = 5;
}

message TokenMetrics {
  int32 prompt_tokens = 1;
  int32 completion_tokens = 2;
  int32 total_tokens = 3;
  float cost = 4;
}

message HealthCheckRequest {}

message HealthCheckResponse {
  string status = 1;
  string version = 2;
  repeated string available_providers = 3;
}
